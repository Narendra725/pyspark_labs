{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ac7d6f60-cec4-484c-9464-61d5f0ebee75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# %pip install semantic-link-labs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "134ed94e-232c-46f8-920e-7dbec6ea71de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "**PROMO SEMANTIC MODEL REFRESH THROUGH PARTITIONS POC**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "46637b61-0362-4e3f-960c-a2611b6e2902",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# labs.list_refresh_requests( dataset='Supply Chain Semantic Model',workspace='ws_sci_sm_qa').dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a96c0da4-13c0-46ec-aab1-de3aaea7ce2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# import sempy.fabric as labs\n",
    "# # labs.list_refresh_requests( dataset='Supply Chain Semantic Model',workspace='ws_sci_sm_qa')\n",
    "# labs.get_refresh_execution_details(dataset='Supply Chain Semantic Model',workspace='ws_sci_sm_qa',refresh_request_id='bedd2011-1f67-4f9d-a926-716db71b8842')\n",
    "# # labs.RefreshExecutionDetails.messages(dataset='Supply Chain Semantic Model',workspace='ws_sci_sm_qa',refresh_request_id='60213e81-97e7-4a6e-a951-2fc3d5e47aef')\n",
    "# # labs.get_refresh_execution_details(dataset='Supply Chain Semantic Model',workspace='ws_sci_sm_qa')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8697ff47-18c5-4101-9bfd-ee4363e429a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# import pkg_resources\n",
    "\n",
    "# for dist in pkg_resources.working_set:\n",
    "#     print(dist.project_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ec80c794-e7a4-4710-bff0-fa2a48932cb5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "import sempy.fabric as labs\n",
    "import time\n",
    "import pandas as pd\n",
    "import pytz\n",
    "from datetime import datetime,timedelta,date\n",
    "from zoneinfo import ZoneInfo\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "487260a2-ecb5-4760-b98b-a437908f2d0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "sqlViewState": {
     "chartOptions": {
      "aggregationType": "sum",
      "binsNumber": 10,
      "categoryFieldKeys": [],
      "chartType": "bar",
      "isStacked": false,
      "seriesFieldKeys": [],
      "wordFrequency": "-1"
     },
     "tableOptions": {},
     "viewOptionsGroup": [
      {
       "tabItems": [
        {
         "key": "0",
         "name": "Table",
         "options": {},
         "type": "table"
        }
       ]
      }
     ]
    }
   },
   "outputs": [],
   "source": [
    "#  %%sql\n",
    "\n",
    "# delete from lh_daily_hist.nmc_all_tab_columns\n",
    "#  where lower(TABLE_NAME) = 'ms_item_ar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bb39d399-da49-421e-99bf-d76b2b157454",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# labs.refresh_dataset(refresh_type= 'full',objects=[{'table' : 'FN Credit Sales'},{'table':'FN Insurance'}],max_parallelism=20,apply_refresh_policy= False,retry_count=2,verbose=3,dataset = 'Finance Semantic Model' , workspace = 'ws_finance_sm_qa')\n",
    "# labs.refresh_semantic_modelrefresh_type= 'full',,tables= ['Pl GM Mon Item Store'],dataset = 'Finance Semantic Model' , workspace = 'ws_finance_sm_qa')\n",
    "\n",
    "# ,objects=[{'table' : 'Sales Day Item'}],commit_mode='partialBatch'\n",
    "# ,objects=[{'table' : 'Location'},{'table' : 'SalesMonthly'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c189307-b06b-4050-9f01-95af1fc29118",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# start_time = datetime.now()\n",
    "# labs.refresh_semantic_model(refresh_type= 'full',tables= ['DateTable'],dataset = 'Supply Chain Semantic Model' , workspace = 'ws_sci_sm_qa')\n",
    "# labs.get_semantic_model_refresh_history(dataset = 'Supply Chain Semantic Model' , workspace = 'ws_sci_sm_qa')\n",
    "# end_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "32839521-9ba8-4ab3-baaf-46cbd722a7a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "def pre_execution_checks (dataset_id,workspace_id):\n",
    "\n",
    "    ## 'worspace_id' check\n",
    "    try :\n",
    "        datasets = labs.list_datasets(workspace = workspace_id)['Dataset Name']\n",
    "    except Exception as e:\n",
    "        return e\n",
    "\n",
    "    ## 'dataset_id' check\n",
    "    if dataset_id in datasets.values:\n",
    "        print(\"dataset_id Check Pass\")\n",
    "    else:\n",
    "        return (\"Dataset '\" + dataset_id + \"' not present in '\" + workspace_id + \"'.\")\n",
    "\n",
    "    ## what to do if there is an ongoing refresh?\n",
    "    return \n",
    "def last_refresh_status (dataset_id,workspace_id):\n",
    "\n",
    "    ## Returns 'False' if it is in Progress else 'True'\n",
    "\n",
    "    pre_execution_checks(dataset_id,workspace_id)\n",
    "    last_refresh = labs.list_refresh_requests(dataset = dataset_id, workspace = workspace_id).head(1)\n",
    "    return ~(last_refresh['End Time'].isna())\n",
    "\n",
    "def wait_till_refresh (dataset_id,workspace_id,request_id):\n",
    "\n",
    "    ## Wait while the model refreshes (as per the sempy.fabric docs 'end_time' remains 'None' till the model refreshes)\n",
    "    current_refresh_details =  labs.list_refresh_requests(dataset = dataset_id, workspace = workspace_id).head(1)['End Time'].values[0]\n",
    "\n",
    "    # print(current_refresh)\n",
    "    while pd.isna(current_refresh_details) :\n",
    "        time.sleep(5)\n",
    "        current_refresh_details =  labs.list_refresh_requests(dataset = dataset_id, workspace = workspace_id).head(1)['End Time'].values[0]\n",
    "    return\n",
    "def dataset_refresh(dataset_id,workspace_id):\n",
    "    \n",
    "    ## perform the necessary checks\n",
    "    pre_execution_checks(dataset_id,workspace_id)\n",
    "\n",
    "    ## Start the refresh\n",
    "\n",
    "    if last_refresh_status :\n",
    "        print(f\"'{dataset_id}' refresh has started\")\n",
    "        try :\n",
    "                request_id = labs.refresh_dataset(refresh_type= 'automatic',objects=[{\"table\": \"DateTable\"}],dataset = dataset_id, workspace =workspace_id,verbose =1 )\n",
    "        except Exception as e:\n",
    "                mssparkutils.notebook.exit(e)\n",
    "        \n",
    "\n",
    "    ## Wait for the refresh to complete\n",
    "    wait_till_refresh (dataset_id,workspace_id,request_id)\n",
    "    print(f\"'{dataset_id}' refresh has completed \")\n",
    "    \n",
    "    return\n",
    "def latest_refresh_stats (dataset_id,workspace_id, get_stats = True,run_refresh = False):\n",
    "\n",
    "    if get_stats :\n",
    "        df_refresh_status = {'Semantic Model' : dataset_id,\n",
    "                             'Refresh Type' : 'Null',\n",
    "                             'Refresh Date' : 'Null',\n",
    "                            'Refresh Start Time (KSA)' :'Null',\n",
    "                            'Duration': 'Null',\n",
    "                            'Status': 'Null',\n",
    "                            'Remarks':'Null',\n",
    "                            'Error Desc': 'Null'}\n",
    "        KSA = pd.Timedelta(hours= 3)\n",
    "        IST = pd.Timedelta(hours = 5, minutes = 30)\n",
    "        today = datetime.today().date()\n",
    "        \n",
    "        df_refresh_hist = labs.list_refresh_requests(dataset = dataset_id, workspace = workspace_id) \n",
    "        \n",
    "        ## Get KSA timestamps for start and end timestamps\n",
    "\n",
    "        df_refresh_hist['KSA Start Time']= pd.to_datetime(df_refresh_hist['Start Time'],utc = True) + KSA\n",
    "        df_refresh_hist['KSA End Time'] =pd.to_datetime(df_refresh_hist['End Time'],utc = True) + KSA\n",
    "        df_refresh_hist['Start Date'] = df_refresh_hist['KSA Start Time'].dt.date\n",
    "        df_refresh_hist['Duration']= df_refresh_hist.apply(lambda x: (x['KSA End Time'] - x['KSA Start Time']).seconds, axis = 1)\n",
    "\n",
    "        ## Get the Expected Model Refresh Duration (excluding refreshes through SSMS as they may not do the 'full' refresh)\n",
    "\n",
    "        df_temp = df_refresh_hist[\n",
    "            (df_refresh_hist['Start Date'] < today) &\n",
    "            (df_refresh_hist['Refresh Type'].isin(['Scheduled', 'OnDemand', 'Via Enhanced Api']))\n",
    "        ]\n",
    "        df_temp = df_temp[df_temp['Status'] == 'Completed']\n",
    "        avg_refresh_duration = df_temp['Duration'].mean()\n",
    "\n",
    "        ## Filter for the current refresh ( we can also get the top 1 from the refreshes)\n",
    "        # df_refresh_current = df_refresh_hist[(df_refresh_hist['KSA Start Time'] >= pd.to_datetime(start_time + KSA,utc= True) )  & ( df_refresh_hist['Refresh Type']== 'ViaEnhancedApi')]\n",
    "        df_refresh_current = df_refresh_hist.head(1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        df_refresh_status['Refresh Type'] = df_refresh_current['Refresh Type'][0]\n",
    "        df_refresh_status['Refresh Date'] = df_refresh_current ['KSA Start Time'].dt.strftime('%Y-%m-%d').values[0]\n",
    "        df_refresh_status['Refresh Start Time (KSA)'] = df_refresh_current['KSA Start Time'].dt.strftime('%I:%M:%S %p').values[0]\n",
    "\n",
    "        ## If the pipeline is run for only stats then don't stop for the refresh to complete, instead go ahead with run in progress as a remark\n",
    "        if pd.isna(df_refresh_current['End Time'].values[0]):\n",
    "            df_refresh_status['Status'] = 'In Progress'\n",
    "            df_refresh_status['Remarks'] = 'üîÑ Refresh is in Progess, Hence skipped the few metrics.'\n",
    "            return df_refresh_status\n",
    "            \n",
    "        df_refresh_status['Status'] = df_refresh_current['Status'][0]\n",
    "      \n",
    "        current_refresh_status = df_refresh_current['Duration'].mean()\n",
    "        ## Model Duration Sanity Check\n",
    "        if 90 <= (current_refresh_status*100.00/avg_refresh_duration) <= 110:\n",
    "                df_refresh_status['Remarks'] = '‚úÖ Refresh Completed in Time'\n",
    "        elif (current_refresh_status*100.00/avg_refresh_duration) < 90 :\n",
    "                df_refresh_status['Remarks'] = '‚úÖ Refresh Completed Quickly'\n",
    "        else  :\n",
    "                df_refresh_status['Remarks'] = 'üü° Refresh Took Longer Than Expected'\n",
    "\n",
    "        if current_refresh_status >= 3600 :\n",
    "             current_refresh_status = f\"{int(current_refresh_status//3600)}h {int((current_refresh_status%3600)//60)} Min {int(current_refresh_status % 60)} Sec\" \n",
    "        else :\n",
    "            current_refresh_status = f\"{int(current_refresh_status//60)} Min {int(current_refresh_status % 60)} Sec\" \n",
    "        ## Duration \n",
    "        df_refresh_status['Duration'] = current_refresh_status \n",
    "\n",
    "        # df_refresh_status['Model Size'] = labs.get_semantic_model_size(dataset=dataset, workspace=workspace) / (1024*1024*1024)\n",
    "\n",
    "\n",
    "        ## Check the Model Referesh if it is failed\n",
    "        df_refresh_current_f = df_refresh_current[df_refresh_current['Status'] == 'Failed']\n",
    "        if len(df_refresh_current_f) > 0:\n",
    "            df_refresh_status['Remarks'] = 'üî¥ Refresh Failed!!'\n",
    "            df_refresh_status['Error Desc'] = df_refresh_current_f['Service Exception Json'][0]\n",
    "\n",
    "        ## Check the Model Referesh if it is Canceled\n",
    "        df_refresh_current_c = df_refresh_current[df_refresh_current['Status'] == 'Cancelled']\n",
    "        if len(df_refresh_current_c) > 0:         \n",
    "            df_refresh_status['Remarks'] = '‚õîÔ∏è Refresh Cancelled by User!!'\n",
    "            df_refresh_status['Error Desc'] = df_refresh_current_f['Service Exception Json'][0]\n",
    "        return df_refresh_status\n",
    "    return \"Stats Not Requested\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3bf2c389-da79-41a2-9f2d-84bf48d97193",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "start_time = datetime.now()\n",
    "\n",
    "if run_refresh == True:\n",
    "    dataset_refresh(dataset,workspace)\n",
    "status = latest_refresh_stats (dataset,workspace, get_stats)\n",
    "if type(status) == dict:\n",
    "    status = f\"<tr><th>{workspace}</th><th> {status['Semantic Model']}</th><th>{status['Refresh Type'] }</th><th>{status['Refresh Date']} </th><th>{status['Refresh Start Time (KSA)']} </th><th>{status['Duration']} </th><th> {status['Status']}</th><th>{status['Remarks']} </th><th>{status['Error Desc']} </th></tr>\"\n",
    "            \n",
    "\n",
    "mssparkutils.notebook.exit(status)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "037f6cc3-97be-45c3-aa0a-207313da17de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# dataset = 'Supply Chain Semantic Model'\n",
    "# workspace = 'ws_sci_sm_prd'\n",
    "# latest_refresh_stats(dataset,workspace,True)\n",
    "# # # .\n",
    "\n",
    "# # latest_refresh_stats (dataset,workspace, True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a5cbfaf0-8762-4d25-908e-006cb0f4b892",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# df_refresh_status = {'Semantic Model' : dataset,\n",
    "#                      'Refresh Start (KSA)' :None,\n",
    "#                      'Duration': None,\n",
    "#                      'Refresh Type' : None,\n",
    "#                      'Status': None,\n",
    "#                      'Remarks':None,\n",
    "#                      'Error Code': None,\n",
    "#                      'Error Desc': None}\n",
    "# KSA = pd.Timedelta(hours= 2,minutes = 55)\n",
    "# IST = pd.Timedelta(hours = 5, minutes = 30)\n",
    "# today = datetime.today().date()\n",
    "# df_refresh_hist = labs.list_refresh_requests(dataset = dataset, workspace = workspace)\n",
    "\n",
    "# ## Get KSA timestamps for start and end timestamps\n",
    "\n",
    "# df_refresh_hist['KSA Start Time']= pd.to_datetime(df_refresh_hist['Start Time'],utc = True) + KSA\n",
    "# df_refresh_hist['KSA End Time'] =pd.to_datetime(df_refresh_hist['End Time'],utc = True) + KSA\n",
    "# df_refresh_hist['Start Date'] = df_refresh_hist['KSA Start Time'].dt.date\n",
    "# df_refresh_hist['Duration']= df_refresh_hist.apply(lambda x: (x['KSA End Time'] - x['KSA Start Time']).seconds, axis = 1)\n",
    "\n",
    "# ## Get the Expected Model Refresh Duration (excluding refreshes through SSMS as they may not do the 'full' refresh)\n",
    "\n",
    "# df_temp = df_refresh_hist[\n",
    "#     (df_refresh_hist['Start Date'] < today) &\n",
    "#     (df_refresh_hist['Refresh Type'].isin(['Scheduled', 'OnDemand', 'Via Enhanced Api']))\n",
    "# ]\n",
    "# df_temp = df_temp[df_temp['Status'] == 'Completed']\n",
    "# avg_refresh_duration = df_temp['Duration'].mean()\n",
    "\n",
    "# ## Filter for the current refresh\n",
    "# df_refresh_current = df_refresh_hist[(df_refresh_hist['KSA Start Time'] >= pd.to_datetime(start_time + KSA,utc= True) ) & ( df_refresh_hist['KSA End Time'] <= pd.to_datetime(end_time+KSA,utc = True) ) & ( df_refresh_hist['Refresh Type']== 'ViaEnhancedApi')]\n",
    "# current_refresh_status = df_refresh_current['Duration'].mean()\n",
    "\n",
    "\n",
    "\n",
    "# ## Model Duration Sanity Check\n",
    "# if len(df_refresh_current) > 0 :\n",
    "#     if 90 <= (current_refresh_status*100.00/avg_refresh_duration) <= 110:\n",
    "#         df_refresh_status['Remarks'] = 'Refresh Completed in Time'\n",
    "#     elif (current_refresh_status*100.00/avg_refresh_duration) < 90 :\n",
    "#         df_refresh_status['Remarks'] = 'Refresh Completed Quickly'\n",
    "#     else  :\n",
    "#         df_refresh_status['Remarks'] = 'Refresh Took Longer Than Expected'\n",
    "\n",
    "#     df_refresh_status['Duration'] = current_refresh_status\n",
    "#     df_refresh_status['Refresh Type'] = 'Via Enhanced Api'\n",
    "\n",
    "# df_refresh_status['Remarks'] = 'Refresh '\n",
    "# # df_refresh_status['Model Size'] = labs.get_semantic_model_size(dataset=dataset, workspace=workspace) / (1024*1024*1024)\n",
    "\n",
    "# ## Check the Model Referesh if it is Completed\n",
    "# df_refresh_current_s = df_refresh_current[df_refresh_current['Status'] == 'Completed']\n",
    "# if len(df_refresh_current_s)> 0 :\n",
    "#     df_refresh_status['Refresh Start (KSA)'] =   df_refresh_current_s['KSA Start Time'][0]\n",
    "#     df_refresh_status['Status']= 'Completed'\n",
    "    \n",
    "\n",
    "# ## Check the Model Referesh if it is failed\n",
    "# df_refresh_current_f = df_refresh_current[df_refresh_current['Status'] == 'Failed']\n",
    "# if len(df_refresh_current_f) > 0:\n",
    "#     df_refresh_status['Refresh Start (KSA)'] =   df_refresh_current_f['KSA Start Time'][0]\n",
    "#     df_refresh_status['Duration'] =  df_refresh_current_f['Duration'][0]\n",
    "#     df_refresh_status['Refresh Type'] = 'Via Enhanced Api'\n",
    "#     df_refresh_status['Status']= 'Failed'\n",
    "#     df_refresh_status['Remarks'] = 'Refresh Failed!!'\n",
    "#     df_refresh_status['Error Code'] = df_refresh_current_f['Error Code'][0],\n",
    "#     df_refresh_status['Error Desc'] = df_refresh_current_f['Error Description'][0]\n",
    "\n",
    "# ## Check the Model Referesh if it is Canceled\n",
    "# df_refresh_current_c = df_refresh_current[df_refresh_current['Status'] == 'Cancelled']\n",
    "# if len(df_refresh_current_c) > 0:\n",
    "#     df_refresh_status['Refresh Start (KSA)'] =   df_refresh_current_c['KSA Start Time'][0]\n",
    "#     df_refresh_status['Duration'] =  df_refresh_current_f['Duration'][0]\n",
    "#     df_refresh_status['Refresh Type'] = 'Via Enhanced Api'\n",
    "#     c\n",
    "#     df_refresh_status['Remarks'] = 'Refresh Unsuccessful!!'\n",
    "#     df_refresh_status['Error Code'] = df_refresh_current_f['Error Code'][0],\n",
    "#     df_refresh_status['Error Desc'] = df_refresh_current_f['Error Description'][0]\n",
    "\n",
    "# mssparkutils.notebook.exit(df_refresh_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6c31b105-8177-4edf-9fe0-5a9c68f68633",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# model_dict = {\n",
    "            #   'FTG Semantic Model'        : { 'dataset' : 'FTG Semantic Model'          , 'workspace' : 'ws_retail_sm_qa' },\n",
    "#             'Global Semantic Model'       : { 'dataset' : 'Global Semantic Model'       , 'workspace' : 'ws_retail_sm_qa' },\n",
    "#             'HR Semantic Model'           : { 'dataset' : 'HR Semantic Model'           , 'workspace' : 'ws_hr_sm_qa' },\n",
    "#             'Supply Chain Semantic Model' : { 'dataset' : 'Supply Chain Semantic Model' , 'workspace' : 'ws_sci_sm_qa' },\n",
    "#             'Initiative Semantic Model'   : { 'dataset' : 'Initiative Semantic Model'   , 'workspace' : 'ws_retail_sm_qa' },\n",
    "#             'Quality Semantic Model'      : { 'dataset' : 'Quality Semantic Model'      , 'workspace' : 'ws_retail_sm_qa' },\n",
    "#             'Sales Semantic Model'        : { 'dataset' : 'Sales Semantic Model'        , 'workspace' : 'ws_retail_sm_qa' },\n",
    "#             'Finance Semantic Model'      : { 'dataset' : 'Finance Semantic Model'      , 'workspace' : 'ws_finance_sm_qa' },\n",
    "#             'Inventory Semantic Model'    : { 'dataset' : 'Inventory_Semantic_Model'    , 'workspace' : 'ws_sci_sm_qa' },\n",
    "#             'Payment Semantic Model'      : { 'dataset' : 'Payment Semantic Model'      , 'workspace' : 'ws_retail_sm_qa' },\n",
    "#             'Clinics Semantic Model'      : { 'dataset' : 'Clinics Semantic Model'      , 'workspace' : 'ws_clinics_sm_qa' }\n",
    "#             }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7857f034-287a-44b7-8562-1dc6e76471fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# print(\"---> Pinging \" + model_dict[k]['dataset'])\n",
    "#     df_time_zone = labs.get_semantic_model_refresh_schedule(dataset=model_dict[k]['dataset'], workspace=model_dict[k]['workspace'])\n",
    "#     df_time_zone = df_time_zone[df_time_zone['Refresh Type'] == 'Scheduled']\n",
    "#     df_time_zone['KSA Start Time']= pd.to_datetime(df_time_zone['Start Time'],utc = True) + KSA\n",
    "#     df_time_zone['KSA End Time'] =pd.to_datetime(df_time_zone['End Time'],utc = True) + KSA\n",
    "#     df_time_zone['IST Start Time']= pd.to_datetime(df_time_zone['Start Time'],utc = True) + IST\n",
    "#     df_time_zone['IST End Time'] =pd.to_datetime(df_time_zone['End Time'],utc = True) + IST\n",
    "#     df_time_zone['Duration']=df.apply(lambda x: (x['IST End Time'] - x['IST Start Time']).seconds, axis = 1)\n",
    "#     model_sizes = pd.concat([\n",
    "#     model_sizes,\n",
    "#     pd.DataFrame({\n",
    "#         'Semantic Model': [model_dict[k]['dataset']],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "312cce9a-1d8c-4911-b9a9-95eb72c0f5ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# labs.refresh_semantic_model(dataset=dataset, workspace=workspace) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "02a705dd-1fe2-46f5-98e5-760c632ab078",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# mssparkutils.notebook.exit((dataset,workspace))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "nb_model_refresh_schedule",
   "widgets": {}
  },
  "dependencies": {
   "lakehouse": {
    "default_lakehouse": "c1ff0d57-698e-4893-b451-4eb064b94c6f",
    "default_lakehouse_name": "lh_dataanalytics_stage",
    "default_lakehouse_workspace_id": "d9efac4c-e5ae-4015-ba5f-c6c5886f9a2f",
    "known_lakehouses": [
     {
      "id": "c1ff0d57-698e-4893-b451-4eb064b94c6f"
     }
    ]
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
