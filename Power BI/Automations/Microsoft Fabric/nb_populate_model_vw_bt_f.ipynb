{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d419f0c-a3a0-471b-b8af-7f0dc723e9fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Notebook Description \n",
    "    1. A shorcut to the Warehouse table 'gold_scin.sql_dependency_map' is created in the lakehouse.\n",
    "    2. A table is created in the lh_dataanalytics_stage using the processing steps that fetch the model-wise tables used in the warehouse.\n",
    "    3. This notebook reads these tables, calculates the 'Root to Leaf Node Paths' for each view used in the different Models and writes the resultant dataframe into table\n",
    "       'lh_dataanalytics.w_sm_wh_vw_lineage'.\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1e24ab66-ba8c-454a-9311-03d0f7edc6ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "70c83de2-03f5-42a5-9848-a9e2e4781ad6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "import sempy.fabric as labs\n",
    "import time\n",
    "import pandas as pd\n",
    "import pytz\n",
    "from datetime import datetime,timedelta,date\n",
    "from zoneinfo import ZoneInfo\n",
    "import json\n",
    "\n",
    "KSA = pd.Timedelta(hours= 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc87536c-e3bf-4264-906a-2806d2f70d5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# import com.microsoft.spark.fabric\n",
    "# from com.microsoft.spark.fabric.Constants import Constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a9e01173-3a89-4532-add5-91d52e85dd83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "<font size=\"5\">**POC I : MODEL LEVEL TABLES & BASE VIEWS FROM WAREHOUSE AUTOMATION**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ec83572b-10b8-412d-bc5e-bd4306ea0c10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "model_dict = {\n",
    "            #  'FTG Semantic Model'        : { 'dataset' : 'FTG Semantic Model'          , 'workspace' : 'ws_retail_sm_qa' }, --> Model Depricated\n",
    "            'Global Semantic Model'       : { 'dataset' : 'Global Semantic Model'       , 'workspace' : 'ws_retail_sm_qa' },\n",
    "            'HR Semantic Model'           : { 'dataset' : 'HR Semantic Model'           , 'workspace' : 'ws_hr_sm_qa' },\n",
    "            'Supply Chain Semantic Model' : { 'dataset' : 'Supply Chain Semantic Model' , 'workspace' : 'ws_sci_sm_qa' },\n",
    "            'Initiative Semantic Model'   : { 'dataset' : 'Initiative Semantic Model'   , 'workspace' : 'ws_retail_sm_qa' },\n",
    "            'Quality Semantic Model'      : { 'dataset' : 'Quality Semantic Model'      , 'workspace' : 'ws_retail_sm_qa' },\n",
    "            'Sales Semantic Model'        : { 'dataset' : 'Sales Semantic Model'        , 'workspace' : 'ws_retail_sm_qa' },\n",
    "            'Finance Semantic Model'      : { 'dataset' : 'Finance Semantic Model'      , 'workspace' : 'ws_finance_sm_qa' },\n",
    "            'Inventory Semantic Model'    : { 'dataset' : 'Inventory_Semantic_Model'    , 'workspace' : 'ws_sci_sm_qa' },\n",
    "            'Payment Semantic Model'      : { 'dataset' : 'Payment Semantic Model'      , 'workspace' : 'ws_retail_sm_qa' },\n",
    "            'Clinics Semantic Model'      : { 'dataset' : 'Clinics Semantic Model'      , 'workspace' : 'ws_clinics_sm_qa' },\n",
    "            'IT Semantic Model'           : { 'dataset' : 'IT Semantic Model'           , 'workspace' : 'ws_it_sm_qa' },\n",
    "            'Secured Semantic Model'      : { 'dataset' : 'Secured Semantic Model'      , 'workspace' : 'ws_secured_qa' },\n",
    "            'Self service semantic model' : { 'dataset' : 'Self service semantic model' , 'workspace' : 'ws_selfservice_sm_qa' },\n",
    "            'Executive Semantic Model'    : { 'dataset' : 'Executive Semantic Model'   , 'workspace' : 'ws_executive_qa' },\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "82f2f23b-795e-4e9a-b2d5-a4ed7123914a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# labs.refresh_dataset(refresh_type = 'full',objects= [{'table' : 'FN CCC'}],dataset= 'Finance Semantic Model',workspace= 'ws_finance_sm_qa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "269b4313-b537-43f2-800b-ae40f4cae572",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "def extract_schema_item(m_code: str,table,partition):\n",
    "    \n",
    "    if m_code is None:\n",
    "        return None\n",
    "\n",
    "    \n",
    "\n",
    "    # # Get the start of the expression\n",
    "    # start = m_code.index(marker) + len(marker)\n",
    "    # remainder = m_code[start:]\n",
    "\n",
    "    # # Extract Schema\n",
    "    # comma_index = remainder.index(\",\")\n",
    "    # schema = remainder[:comma_index].strip()\n",
    "    \n",
    "\n",
    "    # Extract Item\n",
    "    # item_marker = 'Item = \"'\n",
    "    # item_start = remainder.index(item_marker) + len(item_marker)\n",
    "    # item_end = remainder.index('\"', item_start)\n",
    "    # item = remainder[item_start:item_end]\n",
    "\n",
    "    # pattern =  r'Item\\s*=\\s*\"([^\"]+)\"'\n",
    "    # m_code = re.sub(r'\\t+', ' ', m_code.strip())\n",
    "    pattern = r'Item\\s*=\\s*[\"“]([^\"\\n\\r]+)[\"”]'\n",
    "    match = re.search(pattern,m_code)\n",
    "    \n",
    "    if match :\n",
    "       return match.group(1)\n",
    "       \n",
    "    else :\n",
    "        print(f\"No Item found for {table} : partition {partition}\")\n",
    "        print(m_code)\n",
    "        return None\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "522f3ec2-2a6f-4dcd-9f4b-2421fe70e149",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "df_partitions = pd.DataFrame(columns= ['Table Name','Partition Name','Partition Refresh Time','Query','Query Group','Source Schema','Source Table','Model'])\n",
    "for model in model_dict.keys():\n",
    "    print(f\"Executing for {model}\")\n",
    "    ## Get the schema name from the model\n",
    "    df =labs.list_expressions(dataset = model_dict[model]['dataset'] , workspace = model_dict[model]['workspace'])\n",
    "    df = df[df['Name'].str.lower() == 'SchemaSource'.lower()]['Expression']\n",
    "    schema = df.values[0].split(\" \")[0][1:-1]\n",
    "\n",
    "    ## get the partitoins and their source tables \n",
    "    ## The below part works if there are no partitions set on date range.\n",
    "\n",
    "    # df_partitions_temp = labs.list_partitions(dataset = model_dict[model]['dataset'], workspace=  model_dict[model]['workspace'])\n",
    "    # df_partitions_temp = df_partitions_temp[['Table Name', 'Partition Name','Query','Query Group' ]]\n",
    "   \n",
    "\n",
    "    rows =[]\n",
    "    tom_wrapper = labs.TOMWrapper(dataset = model_dict[model]['dataset'] , workspace = model_dict[model]['workspace'],readonly= True)\n",
    "    \n",
    "    for partition in tom_wrapper.all_partitions:\n",
    "        directory = None\n",
    "        refreshed_time = str(partition.get_RefreshedTime())\n",
    "        source_type = str(partition.get_SourceType() )\n",
    "        for item in partition.get_Table().Annotations:\n",
    "                if item.get_Name() == 'TabularEditor_TableGroup':\n",
    "                    directory = item.get_Value()\n",
    "                else :\n",
    "                    pass\n",
    "        if directory == None :\n",
    "            if partition.get_QueryGroup() != None:\n",
    "                directory = str(partition.get_QueryGroup().get_Name())\n",
    "            # else :\n",
    "            #     print(str(partition.get_Name()))\n",
    "                \n",
    "    \n",
    "\n",
    "        if source_type in ('PolicyRangePartitionSource','PolicyRange'):\n",
    "            \n",
    "            rows.append({\n",
    "                'Table Name' : partition.get_Table().get_Name(),\n",
    "                'Partition Name' : partition.get_Name(),\n",
    "                'Query' : partition.get_Source().Partition.Table.get_RefreshPolicy().get_SourceExpression(),\n",
    "                'Query Group' : directory  ,\n",
    "                'Partition Refresh Time' : refreshed_time       \n",
    "                 })\n",
    "        elif source_type in ( 'M' , 'Table'):\n",
    "                rows.append({\n",
    "                    'Table Name' : partition.get_Table().get_Name(),\n",
    "                    'Partition Name' : partition.get_Name(),\n",
    "                    'Query' : partition.get_Source().get_Expression(),\n",
    "                    'Query Group' : directory ,\n",
    "                    'Partition Refresh Time' : refreshed_time        \n",
    "                    })\n",
    "        else :\n",
    "            if not str(partition.get_SourceType()) == 'Calculated':\n",
    "                print(f\"Attention Needed for {partition.get_Name()},parent is {str(partition.get_SourceType())}\")\n",
    "    df_partitions_temp = pd.DataFrame(rows)\n",
    "    df_partitions_temp['Source Schema'] = schema\n",
    "    df_partitions_temp['Model'] = model_dict[model]['dataset']\n",
    "    df_partitions_temp = df_partitions_temp.dropna(subset=['Query'])\n",
    "    df_partitions_temp['Source Table'] = df_partitions_temp.apply(lambda row: extract_schema_item(row['Query'],row['Table Name'],row['Partition Name']),axis = 1)\n",
    "    df_partitions_temp['Partition Refresh Time'] = pd.to_datetime(df_partitions_temp['Partition Refresh Time'],format='%m/%d/%Y %I:%M:%S%p')+ KSA\n",
    "    df_partitions_temp['Partition Refresh Time'] = df_partitions_temp['Partition Refresh Time'].dt.strftime('%m/%d/%Y %I:%M:%S %p')\n",
    "    \n",
    "    df_partitions = pd.concat([df_partitions,df_partitions_temp])\n",
    "        \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ddecc61-7836-41a3-8ce9-948a8ceba1ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "df_new = df_partitions.dropna(subset = ['Source Table'])\n",
    "df_new.drop_duplicates(ignore_index=True)\n",
    "df_new.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9931783e-782d-4ba8-8130-934715c4bb39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "spark_df = spark.createDataFrame(df_new )\n",
    "spark.conf.set(\"spark.sql.parquet.int96RebaseModeInWrite\", \"CORRECTED\")\n",
    "spark_df_new = spark_df.toDF(*[col.replace(\" \", \"_\") for col in spark_df.columns])\n",
    "spark_df_new.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\"lh_dataanalytics_stage.w_sm_wh_vw_lineage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "31873431-841e-45a4-aedb-2bd4a68cdce6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "<font size=\"5\">**POC II : MODEL LEVEL VIEWS & BASE TABLES LINEAGE AUTOMATION**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "12bbb6b7-879b-4a29-84c8-38c7b756f474",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Read the tables into spark dataframes and conver them to pandas DF's\n",
    "df_spark = spark.read.format(\"delta\").load(\"Tables/sql_dependency_map\")\n",
    "df = df_spark.toPandas()\n",
    "\n",
    "df_model_spark = spark.read.format(\"delta\").load(\"Tables/w_sm_wh_vw_lineage\")\n",
    "df_model = df_model_spark.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c5b57399-5ba0-4c47-b66d-c9e2ffc22283",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "## collects the parent and childs into a dict and get the unique name for each table using the schema.table/view_name 2 level namespace\n",
    "\n",
    "parent_child_dict ={}\n",
    "for index, row in df.iterrows():\n",
    "    if pd.isna(row['child_name']) or pd.isna(row['parent_name']) or pd.isna(row['parent_schema']) or pd.isna(row['child_schema_name']):\n",
    "        continue\n",
    "    # print(row['parent_schema'])\n",
    "    parent = row['parent_schema']+\".\"+ row['parent_name']\n",
    "    # print(parent)\n",
    "    # print(row['child_schema_name'])\n",
    "    child = row['child_schema_name']+\".\"+ row['child_name']\n",
    "    \n",
    "    if pd.isna(parent):\n",
    "        continue\n",
    "   \n",
    "    if parent not in parent_child_dict.keys():\n",
    "        parent_child_dict[parent] = [child]\n",
    "    elif parent not in parent_child_dict[parent] and child is not None and child != parent:\n",
    "        parent_child_dict[parent].append(child)\n",
    "    else : \n",
    "        print(f\"something wrong about {parent} : {child}. Here is dict if it helps {parent_child_dict[parent]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "35b7227a-f4f3-49fb-9fc4-cc0b10a7ee4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Finds the Root - Leaf Node Path given a Root_Node and the parent_child_Realtions\n",
    "\n",
    "buff_dict = {}\n",
    "def get_root_leaf_paths(parent_node, parent_child_rel,buffer_dict):\n",
    "    if parent_node in buff_dict.keys():\n",
    "        return buff_dict[parent_node]\n",
    "    else :\n",
    "        if parent_node not in parent_child_rel.keys():\n",
    "            return [parent_node]\n",
    "        else:\n",
    "            paths = []\n",
    "            for child in parent_child_rel[parent_node]:\n",
    "                child_paths = get_root_leaf_paths(child, parent_child_rel, buffer_dict)\n",
    "                for path in child_paths:\n",
    "                    paths.append(parent_node + \" <- \" + path)\n",
    "            buff_dict[parent_node] = paths\n",
    "            return paths\n",
    "\n",
    "root_level_paths = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c84d8eea-6930-43da-b008-038e5da11779",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Select only the required cols and for each View Used in the models , find the Lineage\n",
    "df_model_tables = df_model[['Source_Schema','Source_Table','Model']].copy()\n",
    "df_model_tables['Root_Node'] = df_model_tables['Source_Schema']+\".\"+ df_model_tables['Source_Table']\n",
    "df_model_tables['Paths'] = df_model_tables['Root_Node'].apply(lambda x : get_root_leaf_paths(x, parent_child_dict,buff_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8cd57c43-48e5-4c74-bde2-d9523a2bd38b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Use the Lineage and De-Normalize the paths such that each child used in the Root has different Row\n",
    "flattened_rows = []\n",
    "for index,row in df_model_tables[['Root_Node','Paths','Model']].iterrows():\n",
    "    for p in row['Paths'] :\n",
    "        nodes = p.split(\"<-\")\n",
    "        for i in range(2,len(nodes)+1) :\n",
    "            \n",
    "            flattened_rows.append({\n",
    "                'Root_Node': row['Root_Node'],\n",
    "                'Leaf_Node': nodes[i-1],\n",
    "                'Flattened_Path': \"<-\".join(nodes[:i])\n",
    "\n",
    "            }) \n",
    "\n",
    "\n",
    "df_flat = pd.DataFrame(flattened_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a45fc011-934a-4796-bde1-cc57336feffd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Drop any duplicates present and join the both dataframes to get a single consolidated table \n",
    "df_flat = df_flat.drop_duplicates(ignore_index= True)\n",
    "df_final = pd.merge(df_model_tables,df_flat,on = 'Root_Node',how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d2b0235-09f1-4916-aeef-ac04c2246ec8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Write the results into the table \n",
    "spark_df = spark.createDataFrame(df_final )\n",
    "spark_df_final = spark_df.toDF(*[col.replace(\" \", \"_\") for col in spark_df.columns])\n",
    "spark_df_final.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\"lh_dataanalytics_stage.model_vw_bt_f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca8955ca-3cee-4bf8-adbc-61c8e369a2b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "mssparkutils.notebook.exit('Success')"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "nb_populate_model_vw_bt_f",
   "widgets": {}
  },
  "dependencies": {
   "lakehouse": {
    "default_lakehouse": "c1ff0d57-698e-4893-b451-4eb064b94c6f",
    "default_lakehouse_name": "lh_dataanalytics_stage",
    "default_lakehouse_workspace_id": "d9efac4c-e5ae-4015-ba5f-c6c5886f9a2f",
    "known_lakehouses": [
     {
      "id": "c1ff0d57-698e-4893-b451-4eb064b94c6f"
     }
    ]
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "synapse_pyspark",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
